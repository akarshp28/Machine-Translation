{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "cQESKYGw1xp6"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchtext\n",
    "\n",
    "import pandas as pd\n",
    "from queue import PriorityQueue\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1F6op_f1xp9",
    "outputId": "e2fcb2c5-e90d-414e-9d2c-aa2d003a048f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "mydevice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "IbUybDZ91xp_"
   },
   "outputs": [],
   "source": [
    "SOS_TOKEN = '<sos>'\n",
    "EOS_TOKEN = '<eos>'\n",
    "\n",
    "MAX_LEN = 50\n",
    "\n",
    "def len_filter(example):\n",
    "    return len(example.src) <= MAX_LEN and len(example.tgt) <= MAX_LEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cogsJCg51xqE"
   },
   "source": [
    "### Load dummy number reversal dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L3yyYL6esBZo",
    "outputId": "8e39a467-e8bd-466c-cca6-bba406ce648a"
   },
   "outputs": [],
   "source": [
    "# local path of dummy set\n",
    "train_path = './toy_reversal_export/train/data.txt'\n",
    "dev_path = './toy_reversal_export/dev/data.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "An66dGkt1xqF"
   },
   "outputs": [],
   "source": [
    "src = torchtext.data.Field(\n",
    "    batch_first=True, \n",
    "    include_lengths=True\n",
    "    )\n",
    "tgt = torchtext.data.Field(\n",
    "    batch_first=True, \n",
    "    preprocessing = lambda seq: [SOS_TOKEN] + seq + [EOS_TOKEN]\n",
    "    )\n",
    "\n",
    "data_train = torchtext.data.TabularDataset(\n",
    "        path=train_path, format='tsv',\n",
    "        fields=[('src', src), ('tgt', tgt)],\n",
    "        filter_pred=len_filter\n",
    "    )\n",
    "data_dev = torchtext.data.TabularDataset(\n",
    "        path=dev_path, format='tsv',\n",
    "        fields=[('src', src), ('tgt', tgt)],\n",
    "        filter_pred=len_filter\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading actual EN-IT Dataset below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "3zr76n7W1xqI"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENG</th>\n",
       "      <th>IT</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Ciao!</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Corri!</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Corra!</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Correte!</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Who?</td>\n",
       "      <td>Chi?</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ENG        IT                                             Source\n",
       "0   Hi.     Ciao!  CC-BY 2.0 (France) Attribution: tatoeba.org #5...\n",
       "1  Run!    Corri!  CC-BY 2.0 (France) Attribution: tatoeba.org #9...\n",
       "2  Run!    Corra!  CC-BY 2.0 (France) Attribution: tatoeba.org #9...\n",
       "3  Run!  Correte!  CC-BY 2.0 (France) Attribution: tatoeba.org #9...\n",
       "4  Who?      Chi?  CC-BY 2.0 (France) Attribution: tatoeba.org #2..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " (239088, 3) (68311, 3) (34155, 3)\n",
      "./train.tsv ./test.tsv ./valid.tsv\n"
     ]
    }
   ],
   "source": [
    "# load dataset using pandas\n",
    "df = pd.read_csv('./ita-eng/ita.txt', sep='\\t', header=None)\n",
    "df.columns = ['ENG', 'IT', 'Source']\n",
    "display(df.head())\n",
    "\n",
    "# split data\n",
    "train = df.sample(frac=0.70)\n",
    "test = df.sample(frac=0.20)\n",
    "valid = df.sample(frac=0.10)\n",
    "print('\\n', train.shape, test.shape, valid.shape)\n",
    "\n",
    "# save data\n",
    "train.to_csv('train.tsv', sep = '\\t', index=False)\n",
    "test.to_csv('test.tsv', sep = '\\t', index=False)\n",
    "valid.to_csv('valid.tsv', sep = '\\t', index=False)\n",
    "\n",
    "# load paths\n",
    "train_path = './train.tsv'\n",
    "test_path = './test.tsv'\n",
    "valid_path = './valid.tsv'\n",
    "print(train_path, test_path, valid_path)\n",
    "\n",
    "# create torchtext objects\n",
    "src = torchtext.data.Field(batch_first=True, include_lengths=True)\n",
    "tgt = torchtext.data.Field(batch_first=True, preprocessing = lambda seq: [SOS_TOKEN] + seq + [EOS_TOKEN])\n",
    "\n",
    "data_train = torchtext.data.TabularDataset(\n",
    "        path=train_path, format='tsv',\n",
    "        fields=[('src', src), ('tgt', tgt)],\n",
    "        filter_pred=len_filter)\n",
    "\n",
    "data_test = torchtext.data.TabularDataset(\n",
    "        path=test_path, format='tsv',\n",
    "        fields=[('src', src), ('tgt', tgt)],\n",
    "        filter_pred=len_filter)\n",
    "\n",
    "data_valid = torchtext.data.TabularDataset(\n",
    "        path=valid_path, format='tsv',\n",
    "        fields=[('src', src), ('tgt', tgt)],\n",
    "        filter_pred=len_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 tokens from input vocab:\n",
      " ['<unk>', '<pad>', 'I', 'Tom', 'to', 'you', 'the', 'a', 'is', 'in', 'was', \"I'm\", 'of', 'have', 'You', 'that', 'do', 'for', 'be', 'He']\n",
      "\n",
      "20 tokens from output vocab:\n",
      " ['<unk>', '<pad>', '<eos>', '<sos>', 'Tom', 'di', 'è', 'a', 'non', 'che', 'Io', 'Non', 'un', 'la', 'il', 'ha', 'per', 'in', 'sono', 'una']\n",
      "\n",
      "num training examples: 239086\n",
      "\n",
      "example train data:\n",
      "src:\n",
      " ['Tom', 'said', 'that', 'Mary', 'seemed', 'busy.']\n",
      "tgt:\n",
      " ['<sos>', 'Tom', 'ha', 'detto', 'che', 'Mary', 'sembrava', 'impegnata.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "src.build_vocab(data_train)\n",
    "tgt.build_vocab(data_train)\n",
    "input_vocab = src.vocab\n",
    "output_vocab = tgt.vocab\n",
    "\n",
    "print('20 tokens from input vocab:\\n', list(input_vocab.stoi.keys())[:20])\n",
    "print('\\n20 tokens from output vocab:\\n', list(output_vocab.stoi.keys())[:20])\n",
    "\n",
    "print('\\nnum training examples:', len(data_train.examples))\n",
    "\n",
    "item = random.choice(data_train.examples)\n",
    "print('\\nexample train data:')\n",
    "print('src:\\n', item.src)\n",
    "print('tgt:\\n', item.tgt)\n",
    "\n",
    "#showing output below for toy dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R7ZGeEix1xqN"
   },
   "source": [
    "### Model definition and training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "IDN_VycQ1xqO"
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, myinput, hidden):\n",
    "        embedded = self.embedding(myinput).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=mydevice)\n",
    "\n",
    "    \n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=mydevice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "aA1Xp8Jb1xqR"
   },
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion,\n",
    "          max_length=MAX_LEN, teacher_forcing_ratio=0.5):\n",
    "    \n",
    "    # get an initial hidden state for the encoder\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    # zero the gradients of the optimizers\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    # get the seq lengths, used for iterating through encoder/decoder\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    # create empty tensor to fill with encoder outputs\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=mydevice)\n",
    "\n",
    "    # create a variable for loss\n",
    "    loss = 0\n",
    "    \n",
    "    # pass the inputs through the encoder\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    # create a start-of-sequence tensor for the decoder\n",
    "    decoder_input = torch.tensor([[output_vocab.stoi[SOS_TOKEN]]], device=mydevice)\n",
    "\n",
    "    # set the decoder hidden state to the final encoder hidden state\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    # decide if we will use teacher forcing\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        \n",
    "        topv, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "                \n",
    "        loss += criterion(decoder_output, target_tensor[di].unsqueeze(0))\n",
    "        \n",
    "        if use_teacher_forcing:\n",
    "            decoder_input = target_tensor[di]\n",
    "        \n",
    "        if decoder_input.item() == output_vocab.stoi[EOS_TOKEN]:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "_fvIGMXl1xqT"
   },
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, learning_rate=0.01, teacher_forcing_ratio=0.5):\n",
    "    print(f'Running {n_iters} epochs...')\n",
    "    print_loss_total = 0\n",
    "    print_loss_epoch = 0\n",
    "    \n",
    "    encoder_optim = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optim = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # note batch size of 1, just for simplicity\n",
    "    batch_iterator = torchtext.data.Iterator(\n",
    "        dataset=data_train, batch_size=1,\n",
    "        sort=False, sort_within_batch=True,\n",
    "        sort_key=lambda x: len(x.src),\n",
    "        device=mydevice, repeat=False)\n",
    "    \n",
    "    criterion = nn.NLLLoss()\n",
    "    \n",
    "    for e in range(n_iters):\n",
    "        batch_generator = batch_iterator.__iter__()\n",
    "        step = 0\n",
    "        start = time.time()\n",
    "        for batch in batch_generator:\n",
    "            step += 1\n",
    "            \n",
    "            # get the input and target from the batch iterator\n",
    "            input_tensor, input_lengths = getattr(batch, 'src')\n",
    "            target_tensor = getattr(batch, 'tgt')\n",
    "            \n",
    "            # this is because we're not actually using the batches.\n",
    "            # batch size is 1 and this just selects that first one\n",
    "            input_tensor = input_tensor[0]\n",
    "            target_tensor = target_tensor[0]\n",
    "            \n",
    "            loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optim, decoder_optim, criterion, teacher_forcing_ratio=teacher_forcing_ratio)\n",
    "            print_loss_total += loss\n",
    "            print_loss_epoch += loss\n",
    "            \n",
    "            \n",
    "            if step % print_every == 0:\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_loss_total = 0\n",
    "                t = (time.time() - start) / 60\n",
    "                print(f'step: {step}\\t avg loss: {print_loss_avg:.2f}\\t time for {print_every} steps: {t:.2f} min')\n",
    "                start = time.time()\n",
    "        \n",
    "        print_loss_avg = print_loss_epoch / step\n",
    "        print_loss_epoch = 0\n",
    "        print(f'End of epoch {e}, avg loss {print_loss_avg:.2f}')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1CYrxT_n1xqV"
   },
   "source": [
    "###  Create and train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "bOBbHdMM1xqV"
   },
   "outputs": [],
   "source": [
    "hidden_size = 32\n",
    "encoder1 = EncoderRNN(len(input_vocab), hidden_size).to(mydevice)\n",
    "decoder1 = DecoderRNN(hidden_size, len(output_vocab)).to(mydevice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fCKVT2Y11xqX"
   },
   "source": [
    "Here are some guidelines for how much training to expect. Note that these *guidelines*; they are not exact.\n",
    "\n",
    "Only 1 epoch is needed for the number reversal dataset. This produces near-perfect results, and should take less than 5 minutes to run on a CPU.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on EN-IT Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TCAZ8hbk1xqY",
    "outputId": "9d040289-4d0a-438e-8eed-1cf561d30bbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 1 epochs...\n",
      "step: 1000\t avg loss: 6.39\t time for 1000 steps: 1.97 min\n",
      "step: 2000\t avg loss: 5.57\t time for 1000 steps: 1.95 min\n",
      "step: 3000\t avg loss: 5.33\t time for 1000 steps: 2.00 min\n",
      "step: 4000\t avg loss: 5.27\t time for 1000 steps: 2.04 min\n",
      "step: 5000\t avg loss: 5.13\t time for 1000 steps: 1.92 min\n",
      "step: 6000\t avg loss: 5.11\t time for 1000 steps: 1.99 min\n",
      "step: 7000\t avg loss: 5.09\t time for 1000 steps: 1.92 min\n",
      "step: 8000\t avg loss: 5.08\t time for 1000 steps: 1.94 min\n",
      "step: 9000\t avg loss: 5.01\t time for 1000 steps: 1.91 min\n",
      "step: 10000\t avg loss: 4.99\t time for 1000 steps: 1.89 min\n",
      "step: 11000\t avg loss: 4.93\t time for 1000 steps: 1.92 min\n",
      "step: 12000\t avg loss: 4.88\t time for 1000 steps: 1.91 min\n",
      "step: 13000\t avg loss: 4.86\t time for 1000 steps: 1.91 min\n",
      "step: 14000\t avg loss: 4.91\t time for 1000 steps: 1.91 min\n",
      "step: 15000\t avg loss: 4.91\t time for 1000 steps: 1.97 min\n",
      "step: 16000\t avg loss: 4.89\t time for 1000 steps: 2.01 min\n",
      "step: 17000\t avg loss: 4.84\t time for 1000 steps: 1.97 min\n",
      "step: 18000\t avg loss: 4.88\t time for 1000 steps: 1.96 min\n",
      "step: 19000\t avg loss: 4.83\t time for 1000 steps: 2.01 min\n",
      "step: 20000\t avg loss: 4.81\t time for 1000 steps: 2.03 min\n",
      "step: 21000\t avg loss: 4.78\t time for 1000 steps: 1.93 min\n",
      "step: 22000\t avg loss: 4.86\t time for 1000 steps: 1.95 min\n",
      "step: 23000\t avg loss: 4.83\t time for 1000 steps: 1.94 min\n",
      "step: 24000\t avg loss: 4.77\t time for 1000 steps: 2.04 min\n",
      "step: 25000\t avg loss: 4.78\t time for 1000 steps: 1.99 min\n",
      "step: 26000\t avg loss: 4.79\t time for 1000 steps: 1.94 min\n",
      "step: 27000\t avg loss: 4.83\t time for 1000 steps: 1.96 min\n",
      "step: 28000\t avg loss: 4.77\t time for 1000 steps: 1.97 min\n",
      "step: 29000\t avg loss: 4.76\t time for 1000 steps: 2.00 min\n",
      "step: 30000\t avg loss: 4.80\t time for 1000 steps: 2.01 min\n",
      "step: 31000\t avg loss: 4.78\t time for 1000 steps: 1.92 min\n",
      "step: 32000\t avg loss: 4.74\t time for 1000 steps: 1.92 min\n",
      "step: 33000\t avg loss: 4.77\t time for 1000 steps: 1.93 min\n",
      "step: 34000\t avg loss: 4.76\t time for 1000 steps: 1.91 min\n",
      "step: 35000\t avg loss: 4.77\t time for 1000 steps: 1.98 min\n",
      "step: 36000\t avg loss: 4.75\t time for 1000 steps: 2.05 min\n",
      "step: 37000\t avg loss: 4.71\t time for 1000 steps: 1.97 min\n",
      "step: 38000\t avg loss: 4.66\t time for 1000 steps: 1.99 min\n",
      "step: 39000\t avg loss: 4.72\t time for 1000 steps: 1.99 min\n",
      "step: 40000\t avg loss: 4.63\t time for 1000 steps: 2.04 min\n",
      "step: 41000\t avg loss: 4.69\t time for 1000 steps: 1.99 min\n",
      "step: 42000\t avg loss: 4.70\t time for 1000 steps: 1.99 min\n",
      "step: 43000\t avg loss: 4.63\t time for 1000 steps: 1.98 min\n",
      "step: 44000\t avg loss: 4.62\t time for 1000 steps: 2.08 min\n",
      "step: 45000\t avg loss: 4.68\t time for 1000 steps: 1.96 min\n",
      "step: 46000\t avg loss: 4.66\t time for 1000 steps: 1.97 min\n",
      "step: 47000\t avg loss: 4.72\t time for 1000 steps: 1.93 min\n",
      "step: 48000\t avg loss: 4.68\t time for 1000 steps: 1.98 min\n",
      "step: 49000\t avg loss: 4.60\t time for 1000 steps: 1.91 min\n",
      "step: 50000\t avg loss: 4.58\t time for 1000 steps: 1.95 min\n",
      "step: 51000\t avg loss: 4.70\t time for 1000 steps: 1.95 min\n",
      "step: 52000\t avg loss: 4.69\t time for 1000 steps: 1.99 min\n",
      "step: 53000\t avg loss: 4.64\t time for 1000 steps: 1.94 min\n",
      "step: 54000\t avg loss: 4.68\t time for 1000 steps: 1.96 min\n",
      "step: 55000\t avg loss: 4.65\t time for 1000 steps: 1.95 min\n",
      "step: 56000\t avg loss: 4.55\t time for 1000 steps: 1.86 min\n",
      "step: 57000\t avg loss: 4.60\t time for 1000 steps: 1.88 min\n",
      "step: 58000\t avg loss: 4.61\t time for 1000 steps: 1.89 min\n",
      "step: 59000\t avg loss: 4.63\t time for 1000 steps: 1.95 min\n",
      "step: 60000\t avg loss: 4.60\t time for 1000 steps: 1.92 min\n",
      "step: 61000\t avg loss: 4.58\t time for 1000 steps: 1.90 min\n",
      "step: 62000\t avg loss: 4.51\t time for 1000 steps: 1.89 min\n",
      "step: 63000\t avg loss: 4.54\t time for 1000 steps: 2.19 min\n",
      "step: 64000\t avg loss: 4.51\t time for 1000 steps: 2.30 min\n",
      "step: 65000\t avg loss: 4.52\t time for 1000 steps: 2.23 min\n",
      "step: 66000\t avg loss: 4.50\t time for 1000 steps: 2.14 min\n",
      "step: 67000\t avg loss: 4.54\t time for 1000 steps: 1.88 min\n",
      "step: 68000\t avg loss: 4.55\t time for 1000 steps: 2.21 min\n",
      "step: 69000\t avg loss: 4.59\t time for 1000 steps: 2.26 min\n",
      "step: 70000\t avg loss: 4.54\t time for 1000 steps: 2.30 min\n",
      "step: 71000\t avg loss: 4.52\t time for 1000 steps: 2.30 min\n",
      "step: 72000\t avg loss: 4.50\t time for 1000 steps: 1.99 min\n",
      "step: 73000\t avg loss: 4.51\t time for 1000 steps: 1.89 min\n",
      "step: 74000\t avg loss: 4.52\t time for 1000 steps: 1.89 min\n",
      "step: 75000\t avg loss: 4.43\t time for 1000 steps: 1.90 min\n",
      "step: 76000\t avg loss: 4.49\t time for 1000 steps: 1.93 min\n",
      "step: 77000\t avg loss: 4.45\t time for 1000 steps: 1.90 min\n",
      "step: 78000\t avg loss: 4.48\t time for 1000 steps: 1.90 min\n",
      "step: 79000\t avg loss: 4.49\t time for 1000 steps: 1.90 min\n",
      "step: 80000\t avg loss: 4.46\t time for 1000 steps: 1.87 min\n",
      "step: 81000\t avg loss: 4.48\t time for 1000 steps: 1.90 min\n",
      "step: 82000\t avg loss: 4.47\t time for 1000 steps: 1.97 min\n",
      "step: 83000\t avg loss: 4.44\t time for 1000 steps: 2.00 min\n",
      "step: 84000\t avg loss: 4.45\t time for 1000 steps: 2.05 min\n",
      "step: 85000\t avg loss: 4.49\t time for 1000 steps: 2.00 min\n",
      "step: 86000\t avg loss: 4.50\t time for 1000 steps: 2.03 min\n",
      "step: 87000\t avg loss: 4.47\t time for 1000 steps: 1.97 min\n",
      "step: 88000\t avg loss: 4.42\t time for 1000 steps: 1.95 min\n",
      "step: 89000\t avg loss: 4.42\t time for 1000 steps: 1.93 min\n",
      "step: 90000\t avg loss: 4.46\t time for 1000 steps: 1.95 min\n",
      "step: 91000\t avg loss: 4.41\t time for 1000 steps: 1.94 min\n",
      "step: 92000\t avg loss: 4.40\t time for 1000 steps: 1.96 min\n",
      "step: 93000\t avg loss: 4.38\t time for 1000 steps: 1.96 min\n",
      "step: 94000\t avg loss: 4.33\t time for 1000 steps: 2.00 min\n",
      "step: 95000\t avg loss: 4.38\t time for 1000 steps: 2.01 min\n",
      "step: 96000\t avg loss: 4.38\t time for 1000 steps: 1.98 min\n",
      "step: 97000\t avg loss: 4.37\t time for 1000 steps: 2.00 min\n",
      "step: 98000\t avg loss: 4.35\t time for 1000 steps: 1.96 min\n",
      "step: 99000\t avg loss: 4.36\t time for 1000 steps: 2.03 min\n",
      "step: 100000\t avg loss: 4.33\t time for 1000 steps: 1.98 min\n",
      "step: 101000\t avg loss: 4.32\t time for 1000 steps: 2.00 min\n",
      "step: 102000\t avg loss: 4.33\t time for 1000 steps: 1.97 min\n",
      "step: 103000\t avg loss: 4.29\t time for 1000 steps: 2.01 min\n",
      "step: 104000\t avg loss: 4.32\t time for 1000 steps: 2.02 min\n",
      "step: 105000\t avg loss: 4.34\t time for 1000 steps: 2.06 min\n",
      "step: 106000\t avg loss: 4.30\t time for 1000 steps: 2.01 min\n",
      "step: 107000\t avg loss: 4.35\t time for 1000 steps: 2.08 min\n",
      "step: 108000\t avg loss: 4.37\t time for 1000 steps: 2.08 min\n",
      "step: 109000\t avg loss: 4.30\t time for 1000 steps: 2.10 min\n",
      "step: 110000\t avg loss: 4.26\t time for 1000 steps: 2.09 min\n",
      "step: 111000\t avg loss: 4.29\t time for 1000 steps: 1.99 min\n",
      "step: 112000\t avg loss: 4.27\t time for 1000 steps: 2.03 min\n",
      "step: 113000\t avg loss: 4.28\t time for 1000 steps: 2.05 min\n",
      "step: 114000\t avg loss: 4.27\t time for 1000 steps: 2.00 min\n",
      "step: 115000\t avg loss: 4.26\t time for 1000 steps: 2.01 min\n",
      "step: 116000\t avg loss: 4.27\t time for 1000 steps: 2.03 min\n",
      "step: 117000\t avg loss: 4.28\t time for 1000 steps: 2.04 min\n",
      "step: 118000\t avg loss: 4.28\t time for 1000 steps: 2.01 min\n",
      "step: 119000\t avg loss: 4.23\t time for 1000 steps: 2.06 min\n",
      "step: 120000\t avg loss: 4.22\t time for 1000 steps: 2.02 min\n",
      "step: 121000\t avg loss: 4.27\t time for 1000 steps: 2.04 min\n",
      "step: 122000\t avg loss: 4.25\t time for 1000 steps: 1.95 min\n",
      "step: 123000\t avg loss: 4.23\t time for 1000 steps: 1.87 min\n",
      "step: 124000\t avg loss: 4.25\t time for 1000 steps: 1.82 min\n",
      "step: 125000\t avg loss: 4.15\t time for 1000 steps: 1.80 min\n",
      "step: 126000\t avg loss: 4.19\t time for 1000 steps: 1.80 min\n",
      "step: 127000\t avg loss: 4.22\t time for 1000 steps: 1.81 min\n",
      "step: 128000\t avg loss: 4.24\t time for 1000 steps: 1.86 min\n",
      "step: 129000\t avg loss: 4.15\t time for 1000 steps: 1.84 min\n",
      "step: 130000\t avg loss: 4.23\t time for 1000 steps: 1.85 min\n",
      "step: 131000\t avg loss: 4.22\t time for 1000 steps: 1.82 min\n",
      "step: 132000\t avg loss: 4.24\t time for 1000 steps: 1.82 min\n",
      "step: 133000\t avg loss: 4.24\t time for 1000 steps: 1.82 min\n",
      "step: 134000\t avg loss: 4.14\t time for 1000 steps: 1.83 min\n",
      "step: 135000\t avg loss: 4.23\t time for 1000 steps: 1.83 min\n",
      "step: 136000\t avg loss: 4.11\t time for 1000 steps: 1.84 min\n",
      "step: 137000\t avg loss: 4.18\t time for 1000 steps: 1.85 min\n",
      "step: 138000\t avg loss: 4.23\t time for 1000 steps: 1.84 min\n",
      "step: 139000\t avg loss: 4.18\t time for 1000 steps: 1.80 min\n",
      "step: 140000\t avg loss: 4.19\t time for 1000 steps: 1.84 min\n",
      "step: 141000\t avg loss: 4.18\t time for 1000 steps: 1.83 min\n",
      "step: 142000\t avg loss: 4.12\t time for 1000 steps: 1.86 min\n",
      "step: 143000\t avg loss: 4.14\t time for 1000 steps: 1.85 min\n",
      "step: 144000\t avg loss: 4.15\t time for 1000 steps: 1.82 min\n",
      "step: 145000\t avg loss: 4.17\t time for 1000 steps: 1.82 min\n",
      "step: 146000\t avg loss: 4.13\t time for 1000 steps: 1.82 min\n",
      "step: 147000\t avg loss: 4.15\t time for 1000 steps: 1.86 min\n",
      "step: 148000\t avg loss: 4.09\t time for 1000 steps: 1.86 min\n",
      "step: 149000\t avg loss: 4.13\t time for 1000 steps: 1.81 min\n",
      "step: 150000\t avg loss: 4.16\t time for 1000 steps: 1.84 min\n",
      "step: 151000\t avg loss: 4.06\t time for 1000 steps: 1.82 min\n",
      "step: 152000\t avg loss: 4.10\t time for 1000 steps: 1.86 min\n",
      "step: 153000\t avg loss: 4.10\t time for 1000 steps: 1.82 min\n",
      "step: 154000\t avg loss: 4.12\t time for 1000 steps: 1.81 min\n",
      "step: 155000\t avg loss: 4.08\t time for 1000 steps: 1.89 min\n",
      "step: 156000\t avg loss: 4.12\t time for 1000 steps: 1.94 min\n",
      "step: 157000\t avg loss: 4.05\t time for 1000 steps: 1.94 min\n",
      "step: 158000\t avg loss: 4.05\t time for 1000 steps: 1.92 min\n",
      "step: 159000\t avg loss: 4.13\t time for 1000 steps: 1.91 min\n",
      "step: 160000\t avg loss: 4.08\t time for 1000 steps: 1.94 min\n",
      "step: 161000\t avg loss: 4.06\t time for 1000 steps: 1.95 min\n",
      "step: 162000\t avg loss: 4.03\t time for 1000 steps: 1.91 min\n",
      "step: 163000\t avg loss: 4.11\t time for 1000 steps: 1.97 min\n",
      "step: 164000\t avg loss: 4.05\t time for 1000 steps: 1.91 min\n",
      "step: 165000\t avg loss: 4.10\t time for 1000 steps: 1.91 min\n",
      "step: 166000\t avg loss: 4.01\t time for 1000 steps: 1.89 min\n",
      "step: 167000\t avg loss: 4.07\t time for 1000 steps: 1.96 min\n",
      "step: 168000\t avg loss: 4.11\t time for 1000 steps: 1.85 min\n",
      "step: 169000\t avg loss: 4.06\t time for 1000 steps: 1.88 min\n",
      "step: 170000\t avg loss: 4.11\t time for 1000 steps: 1.87 min\n",
      "step: 171000\t avg loss: 4.02\t time for 1000 steps: 1.86 min\n",
      "step: 172000\t avg loss: 4.07\t time for 1000 steps: 1.86 min\n",
      "step: 173000\t avg loss: 3.99\t time for 1000 steps: 1.93 min\n",
      "step: 174000\t avg loss: 4.08\t time for 1000 steps: 1.97 min\n",
      "step: 175000\t avg loss: 4.03\t time for 1000 steps: 1.91 min\n",
      "step: 176000\t avg loss: 4.09\t time for 1000 steps: 1.93 min\n",
      "step: 177000\t avg loss: 4.00\t time for 1000 steps: 1.90 min\n",
      "step: 178000\t avg loss: 3.98\t time for 1000 steps: 1.94 min\n",
      "step: 179000\t avg loss: 4.02\t time for 1000 steps: 1.92 min\n",
      "step: 180000\t avg loss: 4.04\t time for 1000 steps: 1.97 min\n",
      "step: 181000\t avg loss: 4.00\t time for 1000 steps: 2.05 min\n",
      "step: 182000\t avg loss: 4.03\t time for 1000 steps: 2.00 min\n",
      "step: 183000\t avg loss: 3.99\t time for 1000 steps: 2.03 min\n",
      "step: 184000\t avg loss: 4.14\t time for 1000 steps: 2.07 min\n",
      "step: 185000\t avg loss: 4.00\t time for 1000 steps: 2.03 min\n",
      "step: 186000\t avg loss: 4.01\t time for 1000 steps: 2.08 min\n",
      "step: 187000\t avg loss: 4.02\t time for 1000 steps: 2.05 min\n",
      "step: 188000\t avg loss: 4.04\t time for 1000 steps: 2.06 min\n",
      "step: 189000\t avg loss: 3.98\t time for 1000 steps: 2.14 min\n",
      "step: 190000\t avg loss: 4.09\t time for 1000 steps: 2.20 min\n",
      "step: 191000\t avg loss: 4.01\t time for 1000 steps: 2.05 min\n",
      "step: 192000\t avg loss: 4.00\t time for 1000 steps: 2.05 min\n",
      "step: 193000\t avg loss: 4.04\t time for 1000 steps: 2.12 min\n",
      "step: 194000\t avg loss: 3.99\t time for 1000 steps: 2.07 min\n",
      "step: 195000\t avg loss: 4.09\t time for 1000 steps: 2.14 min\n",
      "step: 196000\t avg loss: 4.03\t time for 1000 steps: 2.13 min\n",
      "step: 197000\t avg loss: 4.02\t time for 1000 steps: 2.09 min\n",
      "step: 198000\t avg loss: 4.00\t time for 1000 steps: 2.01 min\n",
      "step: 199000\t avg loss: 4.01\t time for 1000 steps: 2.09 min\n",
      "step: 200000\t avg loss: 3.97\t time for 1000 steps: 1.95 min\n",
      "step: 201000\t avg loss: 4.00\t time for 1000 steps: 2.00 min\n",
      "step: 202000\t avg loss: 3.90\t time for 1000 steps: 2.08 min\n",
      "step: 203000\t avg loss: 3.97\t time for 1000 steps: 2.04 min\n",
      "step: 204000\t avg loss: 3.98\t time for 1000 steps: 2.05 min\n",
      "step: 205000\t avg loss: 4.03\t time for 1000 steps: 1.99 min\n",
      "step: 206000\t avg loss: 3.94\t time for 1000 steps: 1.96 min\n",
      "step: 207000\t avg loss: 3.94\t time for 1000 steps: 1.97 min\n",
      "step: 208000\t avg loss: 4.01\t time for 1000 steps: 1.97 min\n",
      "step: 209000\t avg loss: 4.02\t time for 1000 steps: 1.95 min\n",
      "step: 210000\t avg loss: 3.99\t time for 1000 steps: 2.01 min\n",
      "step: 211000\t avg loss: 4.00\t time for 1000 steps: 2.00 min\n",
      "step: 212000\t avg loss: 3.96\t time for 1000 steps: 2.04 min\n",
      "step: 213000\t avg loss: 3.95\t time for 1000 steps: 2.11 min\n",
      "step: 214000\t avg loss: 3.93\t time for 1000 steps: 1.99 min\n",
      "step: 215000\t avg loss: 3.95\t time for 1000 steps: 2.07 min\n",
      "step: 216000\t avg loss: 4.06\t time for 1000 steps: 2.05 min\n",
      "step: 217000\t avg loss: 3.91\t time for 1000 steps: 2.04 min\n",
      "step: 218000\t avg loss: 3.86\t time for 1000 steps: 2.07 min\n",
      "step: 219000\t avg loss: 3.94\t time for 1000 steps: 2.05 min\n",
      "step: 220000\t avg loss: 3.92\t time for 1000 steps: 2.08 min\n",
      "step: 221000\t avg loss: 3.94\t time for 1000 steps: 2.09 min\n",
      "step: 222000\t avg loss: 3.92\t time for 1000 steps: 1.95 min\n",
      "step: 223000\t avg loss: 3.94\t time for 1000 steps: 1.97 min\n",
      "step: 224000\t avg loss: 3.94\t time for 1000 steps: 1.94 min\n",
      "step: 225000\t avg loss: 3.95\t time for 1000 steps: 1.96 min\n",
      "step: 226000\t avg loss: 3.98\t time for 1000 steps: 1.95 min\n",
      "step: 227000\t avg loss: 3.94\t time for 1000 steps: 2.00 min\n",
      "step: 228000\t avg loss: 3.93\t time for 1000 steps: 1.95 min\n",
      "step: 229000\t avg loss: 3.98\t time for 1000 steps: 1.96 min\n",
      "step: 230000\t avg loss: 3.92\t time for 1000 steps: 1.92 min\n",
      "step: 231000\t avg loss: 3.91\t time for 1000 steps: 1.93 min\n",
      "step: 232000\t avg loss: 3.94\t time for 1000 steps: 1.98 min\n",
      "step: 233000\t avg loss: 3.93\t time for 1000 steps: 1.98 min\n",
      "step: 234000\t avg loss: 3.90\t time for 1000 steps: 1.96 min\n",
      "step: 235000\t avg loss: 3.90\t time for 1000 steps: 1.94 min\n",
      "step: 236000\t avg loss: 3.95\t time for 1000 steps: 2.00 min\n",
      "step: 237000\t avg loss: 3.91\t time for 1000 steps: 2.05 min\n",
      "step: 238000\t avg loss: 3.91\t time for 1000 steps: 2.01 min\n",
      "step: 239000\t avg loss: 3.95\t time for 1000 steps: 2.09 min\n",
      "End of epoch 0, avg loss 4.34\n"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, decoder1, 1, print_every=1000, teacher_forcing_ratio=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "m2G_5Uwx1xqg"
   },
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LEN):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = torch.tensor([input_vocab.stoi[word] for word in sentence], device=mydevice)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=mydevice)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[output_vocab.stoi[SOS_TOKEN]]], device=mydevice)\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        \n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            next_word = output_vocab.itos[topi.item()]\n",
    "            decoded_words.append(next_word)\n",
    "            if next_word == EOS_TOKEN:\n",
    "                break\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translating some example sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PWPksDgH1xqm",
    "outputId": "19ab0572-2a2c-4aa7-be91-88fdf1084386"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Is', 'it', 'worth', 'fixing?']\n",
      "<sos> È la <eos>\n",
      "\n",
      "['Do', 'you', 'think', 'about', 'Tom', 'a', 'lot?']\n",
      "<sos> Spero che Tom sia un po' <eos>\n",
      "\n",
      "['Does', 'this', 'book', 'belong', 'to', 'you?']\n",
      "<sos> Lui ha a <eos>\n",
      "\n",
      "[\"She's\", 'painting', 'her', 'room', 'white.']\n",
      "<sos> Ci sono la <eos>\n",
      "\n",
      "['Tom', \"doesn't\", 'like', 'green', 'peppers.']\n",
      "<sos> Tom non piace la mia <eos>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This selects 5 random datapoints from the training data and shows the generated sequence\n",
    "\n",
    "for i in range(5):\n",
    "    item = random.choice(data_train.examples)\n",
    "    seq = item.src\n",
    "    print(seq)\n",
    "    words = evaluate(encoder1, decoder1, seq)\n",
    "    print(' '.join(words))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beam Search Algorithm\n",
    "\n",
    "original source:\n",
    "\n",
    "https://github.com/budzianowski/PyTorch-Beam-Search-Decoding/blob/master/decode_beam.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "RM53b6XE1xqo"
   },
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class BeamSearchNode(object):\n",
    "    def __init__(self, hiddenstate, previousNode, wordId, logProb, length):\n",
    "        '''\n",
    "        :param hiddenstate:\n",
    "        :param previousNode:\n",
    "        :param wordId:\n",
    "        :param logProb:\n",
    "        :param length:\n",
    "        '''\n",
    "        self.h = hiddenstate\n",
    "        self.prevNode = previousNode\n",
    "        self.wordid = wordId\n",
    "        self.logp = logProb\n",
    "        self.leng = length\n",
    "\n",
    "    def eval(self, alpha=1.0):\n",
    "        reward = 0\n",
    "        # Add here a function for shaping a reward\n",
    "\n",
    "        return self.logp / float(self.leng - 1 + 1e-6) + alpha * reward\n",
    "    \n",
    "\n",
    "def beam_decode(target_tensor, decoder_hidden, beam_width):\n",
    "    '''\n",
    "    :param target_tensor: target indexes tensor of shape [B, T] where B is the batch size and T is the maximum length of the output sentence\n",
    "    :param decoder_hidden: input tensor of shape [1, B, H] for start of the decoding\n",
    "    :param encoder_outputs: if you are using attention mechanism you can pass encoder outputs, [T, B, H] where T is the maximum length of input sentence\n",
    "    :return: decoded_batch\n",
    "    '''\n",
    "\n",
    "    topk = 1  # how many sentence do you want to generate\n",
    "    decoded_batch = []\n",
    "    \n",
    "    # decoding goes sentence by sentence\n",
    "    for idx in range(target_tensor.size(0)):\n",
    "        \n",
    "        #####################################################################################\n",
    "        # some changes made here from the original version are as follows:\n",
    "        # beam_width is now made a user given input instead of a fixed constant\n",
    "        # unsqueeze part was removed since the original code data format was different\n",
    "        # also unused encoder_output line was removed since attention is not used\n",
    "        #####################################################################################\n",
    "        \n",
    "        # Start with the start of the sentence token\n",
    "        decoder_input = torch.LongTensor([[SOS_token]], device=mydevice)\n",
    "\n",
    "        # Number of sentence to generate\n",
    "        endnodes = []\n",
    "        number_required = min((topk + 1), topk - len(endnodes))\n",
    "\n",
    "        # starting node -  hidden vector, previous node, word id, logp, length\n",
    "        node = BeamSearchNode(decoder_hidden, None, decoder_input, 0, 1)\n",
    "        nodes = PriorityQueue()\n",
    "\n",
    "        # start the queue\n",
    "        nodes.put((-node.eval(), node))\n",
    "        qsize = 1\n",
    "        \n",
    "        # start beam search\n",
    "        while True:\n",
    "            # give up when decoding takes too long\n",
    "            if qsize > 2000: break\n",
    "            \n",
    "            # fetch the best node\n",
    "            score, n = nodes.get()\n",
    "            decoder_input = n.wordid\n",
    "            decoder_hidden = n.h\n",
    "            \n",
    "            if n.wordid.item() == EOS_token and n.prevNode != None:\n",
    "                endnodes.append((score, n))\n",
    "                # if we reached maximum # of sentences required\n",
    "                if len(endnodes) >= number_required:\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            # decode for one step using decoder\n",
    "            decoder_output, decoder_hidden = decoder1(decoder_input, decoder_hidden)\n",
    "            \n",
    "            # PUT HERE REAL BEAM SEARCH OF TOP\n",
    "            log_prob, indexes = torch.topk(decoder_output, beam_width)\n",
    "            \n",
    "            nextnodes = []\n",
    "            for new_k in range(beam_width):\n",
    "                decoded_t = indexes[0][new_k].view(1, -1)\n",
    "                log_p = log_prob[0][new_k].item()\n",
    "                \n",
    "                node = BeamSearchNode(decoder_hidden, n, decoded_t, n.logp + log_p, n.leng + 1)\n",
    "                score = -node.eval()\n",
    "                nextnodes.append((score, node))\n",
    "            \n",
    "            # put them into queue\n",
    "            for i in range(len(nextnodes)):\n",
    "                score, nn = nextnodes[i]\n",
    "                nodes.put((score, nn))\n",
    "                # increase qsize\n",
    "            qsize += len(nextnodes) - 1\n",
    "        \n",
    "        # choose nbest paths, back trace them\n",
    "        if len(endnodes) == 0:\n",
    "            endnodes = [nodes.get() for _ in range(topk)]\n",
    "        \n",
    "        utterances = []\n",
    "        for score, n in sorted(endnodes, key=operator.itemgetter(0)):\n",
    "            utterance = []\n",
    "            \n",
    "            #####################################################################################\n",
    "            # pulling words from output vocabulary\n",
    "            # this is another change that i made from the original code\n",
    "            # the original code was adding the word ID but i am pulling decoded word directly from the dictionary\n",
    "            #####################################################################################\n",
    "            \n",
    "            utterance.append(output_vocab.itos[n.wordid])\n",
    "            \n",
    "            # back trace\n",
    "            while n.prevNode != None:\n",
    "                n = n.prevNode\n",
    "                # same here as above, pulling words from output vocabulary\n",
    "                utterance.append(output_vocab.itos[n.wordid])\n",
    "            \n",
    "            utterance = utterance[::-1]\n",
    "            utterances.append(utterance)\n",
    "            \n",
    "        decoded_batch.append(utterances)\n",
    "\n",
    "    return decoded_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create some evaluation functions which makes use of above \"beam_decode\" function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_beam(encoder, decoder, sentence, max_length=MAX_LEN):\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # most of this code is unchanged        \n",
    "        # target sentence in integer encoded format\n",
    "        input_tensor = torch.tensor([input_vocab.stoi[word] for word in sentence], device=mydevice)\n",
    "        print(input_tensor)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        \n",
    "        encoder_hidden = encoder.initHidden()\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=mydevice)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "                \n",
    "        decoder_hidden = encoder_hidden\n",
    "        \n",
    "        decoder_input = torch.tensor([[output_vocab.stoi[SOS_TOKEN]]], device=mydevice)\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        \n",
    "        #####################################################################################\n",
    "        # we dont need encoder outputs since we are not using Attention mechanism\n",
    "        # hence ignored in beam_decode method\n",
    "        #####################################################################################\n",
    "        \n",
    "        # calling the actual function\n",
    "        decoded_words = beam_decode(input_tensor, decoder_hidden, 10)\n",
    "        \n",
    "        return decoded_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating IT translated Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"You're\", 'not', 'rich.']\n",
      "tensor([ 41,  30, 674])\n",
      "total decoded words: 3\n",
      "['non', 'sei']\n",
      "\n",
      "['I', 'love', 'music,', 'especially', 'rock.']\n",
      "tensor([   2,  132, 7440, 5310, 4049])\n",
      "total decoded words: 5\n",
      "['chiedo', 'che', 'sia']\n",
      "\n",
      "['You', 'are', 'not', 'a', 'child', 'any', 'more.']\n",
      "tensor([ 14,  25,  30,   7, 968, 146, 650])\n",
      "total decoded words: 7\n",
      "['non', 'è', 'ancora', 'un', \"po'\", 'di']\n",
      "\n",
      "['Tom', 'is', 'former', 'CIA.']\n",
      "tensor([   3,    8, 5946, 8398])\n",
      "total decoded words: 4\n",
      "['Tom', 'è', 'la']\n",
      "\n",
      "[\"I'm\", 'almost', 'ready', 'to', 'leave.']\n",
      "tensor([ 11, 290, 286,   4, 464])\n",
      "total decoded words: 5\n",
      "['sono', 'stato', 'a']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    item = random.choice(data_train.examples)\n",
    "    seq = item.src\n",
    "    print(seq)\n",
    "    \n",
    "    words = evaluate_beam(encoder1, decoder1, seq)\n",
    "    print('total decoded words:', len(words))\n",
    "    \n",
    "    decoded_sent = []\n",
    "    for w in words:\n",
    "        for i in w:\n",
    "            for k in i:\n",
    "                if k != '<unk>' and k != '<sos>' and k != '<eos>':\n",
    "                    decoded_sent.append(k)\n",
    "    print(decoded_sent)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "nmt_assignment_NLP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
